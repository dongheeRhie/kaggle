{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6% solution\n",
    "\n",
    "- [해당 깃헙 페이지](https://github.com/rbauld/kaggle/tree/master/quora_question_pair)를 참조했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "- 최대한 많은 변수를 생성하여 xgboost에 투입, Feature Engineering에 공을 많이 들인 케이스\n",
    "- 변수 상세\n",
    "    1. 주요 helper functions\n",
    "    2. N-gram 변수들\n",
    "    3. character 단위 N-gram 변수들\n",
    "    4. Bag of Words 변수들 (2,3의 N-gram 변수들을 모두 모아서, countvectorizer로 TDQM으로 만들고, 그걸 naive-bayse embedding. 효과 좋았음\n",
    "    5. Fuzzy matching features / GoogleNews vector embeddings\n",
    "    6. Magic Feature\n",
    "    7. Graph theory features\n",
    "    8. TFIDF features\n",
    "    9. Glove embedding\n",
    "- 모형 개발 : 'SINGLE'  xgboost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text 사전 정제 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean1(text):\n",
    "    \n",
    "    # 패키지를 로딩하고\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    from nltk.corpus import stopwords\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 만약에 인풋이 null이면 종료합니다.\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # 영어의 stopwords를 볼러옵니다.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # text를 정제합니다.\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # 텍스트에서 문장부호들을 제거합니다.\n",
    "    text = ''.join([c for c in text if c not in punctuation]).lower()\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [w for w in text if not x in stops]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "magic feature만드는 function\n",
    "- 두 문장의 빈도를 구해서 평균/곱/제곱을 구하는 변수.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic1(train_in, test_in):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import timeit\n",
    "    \n",
    "    # 인풋 데이터 copy하기\n",
    "    train_orig = train_in.copy()\n",
    "    test_orig = test_in.copy()\n",
    "    \n",
    "    # train, test 데이터의 question만 가지고 오기\n",
    "    df1 = train_orig[['question1']].copy()\n",
    "    df2 = train_orig[['question2']].copy()\n",
    "    df1_test = test_orig[['question1']].copy()\n",
    "    df2_test = test_orig[['question2']].copy()\n",
    "    \n",
    "    # question2를 불러온 데이터의 컬럼명을 question1으로 변경할 것\n",
    "    df2.rename(columns={\"question2\" : 'question1'}, inplace=True)\n",
    "    df2_test.rename(columns={\"question2\" : 'question1'}, inplace=True)\n",
    "    \n",
    "    # 세로로 병합한 후 question이 중복된 것 제거\n",
    "    train_questions = df1.append(df2)\n",
    "    train_questions = train_questions.append(df1_test)\n",
    "    train_questions = train_questions.append(df2_test)\n",
    "    train_questions.drop_duplicates(subset = ['question1'], inplace=True)\n",
    "    \n",
    "    # 인덱스를 컬럼으로 변환\n",
    "    train_questions.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # 질문텍스트 : 인덱스로 대응되는 딕셔너리 만들기 \n",
    "    questions_dict = pd.Series(train_questions.index.values, index = train_questions.question1.values).to_dict()\n",
    "    \n",
    "    #train, test셋을 위아래로 concat\n",
    "    train_cp = train_orig.copy()\n",
    "    test_cp = test_orig.copy()\n",
    "    train_cp.drop(['qid1', 'qid2'], axis=1, inplace=True)\n",
    "    test_cp['is_duplicate'] = -1\n",
    "    test_cp.rename(columns={'test_id':'id'}, inplace=True)\n",
    "    comb = pd.concat([train_cp, test_cp])\n",
    "    \n",
    "    # 두 질문 텍스트를 index 번호로 변경\n",
    "    comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "    comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "    \n",
    "    # 질문별 갯수를 딕셔너리로 생성\n",
    "    q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "    q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "    \n",
    "    # 딕셔너리에서 찾아보고 없으면 0을 출력하는 함수\n",
    "    def try_apply_dict(x, dict_to_apply):\n",
    "        try:\n",
    "            return dict_to_apply[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    \n",
    "    # 전체 문장 등장 빈도 구하기\n",
    "    comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x, q1_vc) + try_apply_dict(x, q2_vc) )\n",
    "    comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x, q1_vc) + try_apply_dict(x, q2_vc) )\n",
    "   \n",
    "    # 파생 변수 계산하기\n",
    "    comb['freq_mean'] = (comb['q1_freq'] + comb['q2_freq']) / 2\n",
    "    comb['freq_cross'] = comb['q1_freq'] * comb['q2_freq']\n",
    "    comb['q1_freq_sq'] = comb['q1_freq'] * comb['q1_freq']\n",
    "    comb['q2_freq_sq'] = comb['q2_freq'] * comb['q2_freq']\n",
    "    \n",
    "    ret_cols = ['id', 'q1_freq', 'q2_freq', 'freq_mean', 'freq_cross', 'q1_freq_sq', 'q2_freq_sq']\n",
    "    \n",
    "    train_comb = comb[comb['is_duplicate'] >= 0][ret_cols]\n",
    "    test_comb = comb[comb['is_duplicate'] < 0][ret_cols]\n",
    "    \n",
    "    return (train_comb[ret_cols], test_comb[ret_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복어휘 매핑하는 인풋 개발"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordmatch1(train_in, test_in, qcolumns = ['question1', 'question2'], append = ''):\n",
    "    \n",
    "    train_df = train_in.copy()\n",
    "    test_df = test_in.copy()\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    def word_match_share(row):\n",
    "        \n",
    "        q1words = {}\n",
    "        q2words = {}\n",
    "        \n",
    "        for word in str(row[qcolumns[0]]).lower().split():\n",
    "            if word not in stops:\n",
    "                q1words[word] = 1\n",
    "        for word in str(row[qcolumns[1]]).lower().split():\n",
    "            if word not in stops:\n",
    "                q2words[word] = 1\n",
    "                \n",
    "        if len(q1words) == 0 or len(q2words) == 0:\n",
    "            # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "            return 0\n",
    "        \n",
    "        shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "        shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "        R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "        return R\n",
    "    \n",
    "    train_df['wordmatch1'+append] = train_df.apply(word_match_share, axis=1)\n",
    "    test_df['wordmatch1'+append] = test_df.apply(word_match_share, axis=1)\n",
    "    \n",
    "    return (train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram 변수 개발"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_stats1(train_in, test_in, qcolumns = ['question1', 'question2'], append=''):\n",
    "    \n",
    "    train_in = train_in.copy()\n",
    "    test_in = test_in.copy()\n",
    "    \n",
    "    # 질문별 길이 및 길이의 차이의 절대값을 변수화\n",
    "    train_in['q1_len' + append] = train_in.apply(lambda x:len(x[qcolumns[0]]), axis=1)\n",
    "    train_in['q2_len' + append] = train_in.apply(lambda x:len(x[qcolumns[1]]), axis=1)\n",
    "    train_in['len_diff'+append] = abs(train_in['q1_len' + append] - train_in['q2_len'+append])\n",
    "    \n",
    "    test_in['q1_len' + append] = test_in.apply(lambda x:len(x[qcolumns[0]]), axis=1)\n",
    "    test_in['q2_len' + append] = test_in.apply(lambda x:len(x[qcolumns[1]]), axis=1)\n",
    "    test_in['len_diff'+append] = abs(test_in['q1_len' + append] - test_in['q2_len'+append])\n",
    "\n",
    "    # N-gram을 변수화해보자\n",
    "    from nltk import ngrams\n",
    "    from collections import Counter\n",
    "    import pandas as pd\n",
    "    from nltk.metrics import distance\n",
    "    \n",
    "    def get_ngram_stats(row, n, qcolumns, char=False, append=''):\n",
    "        \n",
    "        # 캐릭터단위 N-gram이면 텍스트를 공백없이 붙이고, word 단위라면 단어별로 구분\n",
    "        if char==True:\n",
    "            q1 = ''.join(row[qcolumns[0]].split())\n",
    "            q2 = ''.join(row[qcolumns[1]].split())\n",
    "        else:\n",
    "            q1 = row[qcolumns[0]].split()\n",
    "            q2 = row[qcolumns[1]].split()\n",
    "            \n",
    "        # n그램의 리스트 만들기\n",
    "        q1_ngram_list = list(ngrams(q1, n))\n",
    "        q2_ngram_list = list(ngrams(q2, n))\n",
    "         \n",
    "        # 중복 없는 set 만들기\n",
    "        q1_ngram_set = set(q1_ngram_list)\n",
    "        q2_ngram_set = set(q2_ngram_list)\n",
    "        \n",
    "        # ngram의 길이 구하기\n",
    "        q1_sum = len(q1_ngram_list)\n",
    "        q2_sum = len(q2_ngram_list)\n",
    "        \n",
    "        # 길이 차이 구하기\n",
    "        diff = abs(q1_sum - q2_sum)\n",
    "        \n",
    "        #  diff_norm 변수 만들기\n",
    "        if q1_sum + q2_sum != 0:\n",
    "            diff_norm = diff / (q1_sum + q2_sum)*2\n",
    "        else:\n",
    "            diff_norm = -1\n",
    "            \n",
    "        # 두 list의 길이 중 긴것, 작은 것 찾기\n",
    "        maximum = max([q1_sum, q2_sum])\n",
    "        minimum  = min([q1_sum, q2_sum])\n",
    "    \n",
    "        # 두 set의 길이를 구하기\n",
    "        q1_unique = len(q1_ngram_set)\n",
    "        q2_unique = len(q2_ngram_set)\n",
    "        \n",
    "        # set의 길이 차이를 변수화\n",
    "        diff_unique = abs(q1_unique - q2_unique)\n",
    "        \n",
    "        # 두 문장의 N-gram중 중복되는 부분 구하기\n",
    "        intersect_r = Counter(q1_ngram_list) & Counter(q2_ngram_list)\n",
    "        \n",
    "        # 전체 단어의 길이 대비 중복된 단어의 비중 구하기 (List 기준, Set 기준), Masi Distance 구하기\n",
    "        if q1_sum + q2_sum != 0:\n",
    "            intersect_r = sum(intersect_r.values()) / (q1_sum + q2_sum) * 2\n",
    "            intersect_unique_r = len(q1_ngram_set.intersection(q2_ngram_set)) / (q1_unique + q2_unique) *2\n",
    "            masi_dist = distance.masi_distance(q1_ngram_set, q2_ngram_set)\n",
    "        else:\n",
    "            intersect = -1\n",
    "            intersect_unique = -1\n",
    "            masi_dist = -1\n",
    "            \n",
    "        # jaccard distance 구하기\n",
    "        if 0 != len(q1_ngram_set.union(q1_ngram_set)):\n",
    "            jaccard_dist = ( len(q1_ngram_set.union(q2_ngram_set)) - len(q1_ngram_set.intersection(q2_ngram_set)))/ \\\n",
    "                                     len(q1_ngram_set.union(q2_ngram_set)\n",
    "                                    )\n",
    "        else:\n",
    "            jaccard_dist = 1\n",
    "            \n",
    "        # binary distance 구하기\n",
    "        bin_dist = distance.binary_distance(q1_ngram_set, q2_ngram_set)\n",
    "        \n",
    "        listout = [q1_sum  , q2_sum  , diff  ,diff_norm   ,maximum, minimum, \n",
    "                       q1_unique, q2_unique, diff_unique, intersect_r, \n",
    "                       intersect_unique_r, jaccard_dist, bin_dist, masi_dist]\n",
    "    \n",
    "        return listout\n",
    "    \n",
    "    keys = ['q1_sum', 'q2_sum', 'diff', 'diff_norm', 'max', 'min', \n",
    "                 'q1_uni', 'q2_uni', 'diff_uni','intersect_r', 'inter_uni_r',\n",
    "                 'jaccard_dist', 'bin_dist', 'masi_dist']  \n",
    "        \n",
    "        \n",
    "    for n in range(1,4):\n",
    "        print(n)\n",
    "        ngram_stats = train_in.apply(lambda x:get_ngram_stats(x, n=n, qcolumns = qcolumns, char=char), axis=1)\n",
    "        ngram_stats = np.vstack(ngram_stats.values)\n",
    "        keys_tmp = [x+str(n)+append for x in keys]\n",
    "        ngram_stats = pd.DataFrame(ngram_stats, columns = keys_tmp, index=train_in.index)\n",
    "        train_in = train_in.combine_first(ngram_stats) # 먼저 들어온 데이터를 우선으로, null값 있는 경우 뒤에 추가한 데이터로 대체하기\n",
    "        \n",
    "    \n",
    "    for n in range(1,4):\n",
    "        print(n)\n",
    "        ngram_stats = test_in.apply(lambda x:get_ngram_stats(x, n=n, qcolumns = qcolumns, char=char), axis=1)\n",
    "        ngram_stats = np.vstack(ngram_stats.values)\n",
    "        keys_tmp = [x+str(n)+append for x in keys]\n",
    "        ngram_stats = pd.DataFrame(ngram_stats, columns = keys_tmp, index=test_in.index)\n",
    "        test_in = test_in.combine_first(ngram_stats) # 먼저 들어온 데이터를 우선으로, null값 있는 경우 뒤에 추가한 데이터로 대체하기\n",
    "        \n",
    "    return (train_in, test_in)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein distance\n",
    "def edit_distance(train_in, test_in, qcolumns = ['question1', 'question2'], append=''):\n",
    "\n",
    "    train = train_in.copy().loc[:,qcolumns]\n",
    "    test = test_in.copy().loc[:,qcolumns]\n",
    "    \n",
    "    import editdistance\n",
    "    \n",
    "    def my_fun(row, qcolumns):\n",
    "        return editdistance.eval(row[qcolumns[0]], row[qcolumns[1]])\n",
    "    \n",
    "    key = 'edit_dist'+append\n",
    "    train[key] = train.apply(lambda x: my_fun(x, qcolumns=qcolumns), axis=1)\n",
    "    test[key]  = test.apply(lambda x: my_fun(x, qcolumns=qcolumns), axis=1)\n",
    "    \n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein distance를 기반으로 두 문장의 유사도 구하기\n",
    "def fuzzy_feats(train_in, test_in, qcolumns = ['question1', 'question2'], append=''):\n",
    "    from fuzzywuzzy import fuzz\n",
    "    import pandas as pd\n",
    "    \n",
    "    train = train_in.copy().loc[:,qcolumns]\n",
    "    test = test_in.copy().loc[:,qcolumns]\n",
    "    \n",
    "    train['fuzz_r'+append] = train.apply(lambda x: fuzz.ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)\n",
    "    train['fuzz_pr'+append] = train.apply(lambda x: fuzz.partial_ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)\n",
    "    train['fuzz_tsr'+append] = train.apply(lambda x: fuzz.partial_token_set_ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)\n",
    "    train['fuzz_tsor'+append] = train.apply(lambda x: fuzz.partial_token_sort_ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)    \n",
    "    \n",
    "    test['fuzz_r'+append] = test.apply(lambda x: fuzz.ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)\n",
    "    test['fuzz_pr'+append] = test.apply(lambda x: fuzz.partial_ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)\n",
    "    test['fuzz_tsr'+append] = test.apply(lambda x: fuzz.partial_token_set_ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)\n",
    "    test['fuzz_tsor'+append] = test.apply(lambda x: fuzz.partial_token_sort_ratio(x[qcolumns[0]],x[qcolumns[1]]), axis = 1)     \n",
    "    \n",
    "    return (train, test)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
