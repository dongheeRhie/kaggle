{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feat_gen # 변수 생성 펑션 파일\n",
    "import importlib; importlib.reload(feat_gen)\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv(\"../../../Kaggle_IO/QuoraQuestionPairs/input/train.csv\")\n",
    "test_orig = pd.read_csv(\"../../../Kaggle_IO/QuoraQuestionPairs/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_orig.copy()\n",
    "test_df = test_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간에 pickle로 한번 저장하기 (Feature 6까지 저장되어 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data....\n"
     ]
    }
   ],
   "source": [
    "# print('saving data....')\n",
    "# f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F2.pickle', 'wb') \n",
    "# pickle.dump(train_df, f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F2.pickle', 'wb') \n",
    "# pickle.dump(test_df, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드하기 (feature 만드는 중간중간 저장하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F3.pickle', 'rb')\n",
    "train_df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F3.pickle', 'rb')\n",
    "test_df = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['question1'].fillna('', inplace=True)\n",
    "train_df['question2'].fillna('', inplace=True)\n",
    "\n",
    "test_df['question1'].fillna('', inplace=True)\n",
    "test_df['question2'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['q1_clean1'] = train_df.apply(lambda x: feat_gen.clean1(x['question1']), axis=1)\n",
    "train_df['q2_clean1'] = train_df.apply(lambda x: feat_gen.clean1(x['question2']), axis=1)\n",
    "\n",
    "test_df['q1_clean1'] = test_df.apply(lambda x: feat_gen.clean1(x['question1']), axis=1)\n",
    "test_df['q2_clean1'] = test_df.apply(lambda x: feat_gen.clean1(x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 1 :Magic Feature 만들기\n",
    "\n",
    "- 각 문장이 전체에서 발생한 빈도를 구한 후, 각각의 평균/제곱/곱을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_1, test_1) = feat_gen.magic1(train_df, test_df)\n",
    "train_df = train_df.combine_first(train_1)\n",
    "test_df = test_df.combine_first(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_1, test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 2 : Wordmatch Feature 만들기\n",
    "\n",
    "- 두 문장간 중복되는 단어의 비중 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_1, test_1) = feat_gen.wordmatch1(train_df, test_df)\n",
    "(train_2, test_2) = feat_gen.wordmatch1(train_df, test_df, qcolumns = ['q1_clean1','q2_clean1'], append='_c')\n",
    "\n",
    "train_df = train_df.combine_first(train_1)\n",
    "test_df  = test_df.combine_first(test_1)\n",
    "\n",
    "train_df = train_df.combine_first(train_2)\n",
    "test_df  = test_df.combine_first(test_2)\n",
    "del train_1, test_1, train_2, test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns\n",
    "test_df = test_df.drop('id',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 3 : Ngram Feature 만들기\n",
    "\n",
    "- jaccard distance : 1 - (Intersection / Union)\n",
    "- binary distance : 2진법으로 변환한 뒤, 각 자리별 일치/차이로 거리 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "(train_1, test_1) = feat_gen.ngram_stats2(train_df, test_df)\n",
    "(train_2, test_2) = feat_gen.ngram_stats2(train_df, test_df, qcolumns = ['q1_clean1','q2_clean1'], append='_c')\n",
    "\n",
    "train_df = train_df.combine_first(train_1)\n",
    "test_df  = test_df.combine_first(test_1)\n",
    "\n",
    "train_df = train_df.combine_first(train_2)\n",
    "test_df  = test_df.combine_first(test_2)\n",
    "del train_1, test_1, train_2, test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 4 :Levenshtein distance 구하기\n",
    " - Levenshtein distance에 대해서는 [해당 블로그](http://timewizhan.tistory.com/entry/Levenshtein-Distance)를 참고하세요.\n",
    " - 양쪽 문자를 비교, 삭제/추가/변경해야 할때 cost를 1씩 증가시킴, 최종적인 cost가 그 거리가 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_1, test_1) = feat_gen.edit_distance(train_df, test_df)\n",
    "(train_2, test_2) = feat_gen.edit_distance(train_df, test_df, qcolumns = ['q1_clean1','q2_clean1'], append='_c')\n",
    "\n",
    "train_df = train_df.combine_first(train_1)\n",
    "test_df  = test_df.combine_first(test_1)\n",
    "\n",
    "train_df = train_df.combine_first(train_2)\n",
    "test_df  = test_df.combine_first(test_2)\n",
    "\n",
    "del train_1, test_1, train_2, test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 5 : fuzzy feature 구하기 (너무 오래 걸려서 제외) \n",
    "\n",
    "- [공식 블로그](https://github.com/seatgeek/fuzzywuzzy)에 사례로 잘 설명해 두었습니다.\n",
    "- 다양한 방식으로 두 문장의 유사도를 비율로 표현해줍니다.\n",
    "- python-Levenshtein을 설치하지 않고 실행시 정말 **매우매우매우** 오래 걸립니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "\n",
    "# (train_1, test_1) = feat_gen.fuzzy_feats(train_df, test_df)\n",
    "# (train_2, test_2) = feat_gen.fuzzy_feats(train_df, test_df, qcolumns = ['q1_clean1','q2_clean1'], append='_c')\n",
    "\n",
    "# train_df = train_df.combine_first(train_1)\n",
    "# test_df  = test_df.combine_first(test_1)\n",
    "\n",
    "# train_df = train_df.combine_first(train_2)\n",
    "# test_df  = test_df.combine_first(test_2)\n",
    "\n",
    "# del train_1, test_1, train_2, test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 6 : 캐릭터 단위 N-gram 피쳐 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "(train_1, test_1) = feat_gen.ngram_stats2(train_df, test_df, append = 'char', char=True)\n",
    "(train_2, test_2) = feat_gen.ngram_stats2(train_df, test_df, qcolumns = ['q1_clean1','q2_clean1'], append='char_c', char=True)\n",
    "\n",
    "train_df = train_df.combine_first(train_1)\n",
    "test_df  = test_df.combine_first(test_1)\n",
    "\n",
    "train_df = train_df.combine_first(train_2)\n",
    "test_df  = test_df.combine_first(test_2)\n",
    "del train_1, test_1, train_2, test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재까지 만든 변수 정리\n",
    "\n",
    "- Magic Features : 전체에서의 해당 문장의 빈도를 구한 뒤, 해당 빈도의 제곱 / 곱 / 평균을 구함\n",
    "- WordMatch Features : 두 문장의 중복되는 단어의 비중\n",
    "- Ngram Features : 단어 기준으로 문장별로 1~3개로 N-gram을 만든 뒤, 문장의 길이 / 중복된 토큰의 비중 / 각종 거리지표들 (자카드, binary, MASI)을 구함\n",
    "- Char-base Ngram : 같은 과정을 캐릭터 기반으로 수행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__지금까지 만든 변수로 모형을 만들어보기__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id', 'is_duplicate', 'qid1', 'qid2'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train에만 있는 변수 확인\n",
    "set(train_df.columns) - set(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target변수 새로 생성\n",
    "train_df_target = train_df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train에만 있는 쓸모없는 변수 제거\n",
    "train_df.drop([\"id\", \"is_duplicate\", \"qid1\", \"qid2\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test id 제거\n",
    "test_df.drop(['test_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 text 제거\n",
    "train_df.drop(['question1', 'question2'], inplace=True, axis=1)\n",
    "test_df.drop(['question1', 'question2'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 text 제거\n",
    "train_df.drop(['q1_clean1', 'q2_clean1'], inplace=True, axis=1)\n",
    "test_df.drop(['q1_clean1', 'q2_clean1'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비율 조정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos = train_df[train_df_target==1]\n",
    "df_train_neg = train_df[train_df_target==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg = pd.concat([df_train_neg, df_train_neg], axis=0)\n",
    "df_train_neg = pd.concat([df_train_neg, df_train_neg.iloc[:210000,:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720054, 190)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del train_df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([df_train_pos, df_train_neg])\n",
    "train_df_target = Series((np.zeros(len(df_train_pos)) + 1).tolist() + np.zeros(len(df_train_neg)).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost 적합해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정하기\n",
    "\n",
    "params = {\"learning_rate\" : 0.02,\n",
    "                 \"n_estimators\" : 3000,\n",
    "                  \"max_depth\" : 8,\n",
    "                  \"colsample_bytree\" : 0.6,\n",
    "                  \"gamma\" : 2,\n",
    "                  \"lambda\" : 0,\n",
    "                  'alpha' : 0.5,\n",
    "                  \"n_jobs\" : 14,\n",
    "                  \"objective\" : \"binary:logistic\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df, train_df_y, test_size = 0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.680083\tvalidation_1-logloss:0.6801\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 100 rounds.\n",
      "[500]\tvalidation_0-logloss:0.215307\tvalidation_1-logloss:0.222552\n",
      "[1000]\tvalidation_0-logloss:0.197389\tvalidation_1-logloss:0.211657\n",
      "[1500]\tvalidation_0-logloss:0.183753\tvalidation_1-logloss:0.204125\n",
      "[2000]\tvalidation_0-logloss:0.172506\tvalidation_1-logloss:0.198128\n",
      "[2500]\tvalidation_0-logloss:0.163105\tvalidation_1-logloss:0.193131\n",
      "[2999]\tvalidation_0-logloss:0.153899\tvalidation_1-logloss:0.188375\n",
      "0 finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-ae115e5ced6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                                         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                                         verbose=500)\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mcolname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xgb_\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost-0.7-py3.5.egg\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[0;32m    504\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost-0.7-py3.5.egg\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost-0.7-py3.5.egg\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost-0.7-py3.5.egg\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m--> 898\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m    899\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train,\n",
    "        eval_set = [(X_train,y_train), (X_test,y_test)],\n",
    "                            eval_metric='logloss',\n",
    "                            early_stopping_rounds=100,\n",
    "                            verbose=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# single model submission, 0.21059, 729th, 상위 22%\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
