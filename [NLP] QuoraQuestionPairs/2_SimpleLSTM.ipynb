{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs _ simple LSTM\n",
    "\n",
    "- Word2Vec으로 사전 학습된 Word Vector 활용, LSTM 모형을 만들어봅니다. lystdo의 [해당 커널](https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings) 을 주로 참조했습니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. loading packages, import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM모형의 기본 파라미터를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30 # 최대 시퀀스 길이\n",
    "MAX_NB_WORDS = 200000 # 최대 어휘 갯수 (학습 대상이 되는 어휘)\n",
    "EMBEDDING_DIM = 300 # Word Vector의 차원수\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = np.random.randint(175, 275) # LSTM의 차원수\n",
    "num_dense = np.random.randint(100, 150) # Dense Layer의 차원수\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrained word2vec vector를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('../../Analysis/WordVectors/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정리하기 위한 function 불러오기\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    \n",
    "    # 소문자로 변환 후 공백 기준으로 분리\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # stopwords를 불러온 후 stopwords 아닌 단어만 리스트에 넣기\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        \n",
    "    # 어휘들을 공백으로 합치기\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # 각종.....각종 변환 및 정제..\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # 영단어를 어간으로 변환 후 다시 공백으로 합치기\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 불러오기\n",
    "\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "labels = []\n",
    "\n",
    "# 파일을 한 줄씩 열고\n",
    "with codecs.open('../../Kaggle_IO/QuoraQuestionPairs/input/train.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    # text1, text2 각각 text_to_wordlist 펑션으로 가공하여 리스트에 append\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3])) # train셋의 Text 1\n",
    "        texts_2.append(text_to_wordlist(values[4])) # train셋의 Text 2\n",
    "        labels.append(int(values[5]))\n",
    "        \n",
    "# 테스트 데이터도 똑같이 반복        \n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "\n",
    "with codecs.open('../../Kaggle_IO/QuoraQuestionPairs/input/test.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    \n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1])) # test셋의 Text 1\n",
    "        test_texts_2.append(text_to_wordlist(values[2])) # test셋의 Text 2\n",
    "        test_ids.append(values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제한 Word 단위로 Tokenizer 실행하여 단어별 dictionary 구성\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 단어를 index로 변환하여 sequence로 변환\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer의 word_index 가져오기\n",
    "\n",
    "word_index = tokenizer.word_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence에 길이에 맞게 padding하여 구성\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained embedding matrix 가져오기\n",
    "\n",
    "nb_words =min(MAX_NB_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 vaild 구분\n",
    "\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1) * (1-VALIDATION_SPLIT))]\n",
    "idx_valid = perm[int(len(data_1) * (1-VALIDATION_SPLIT)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번 문장과 2번 문장 / 2번 문장과 1번 문장을 번갈아가며 넣는 set 구성\n",
    "\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_valid], data_2[idx_valid]))\n",
    "data_2_val = np.vstack((data_2[idx_valid], data_1[idx_valid]))\n",
    "labels_val = np.concatenate((labels[idx_valid], labels[idx_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight 보정\n",
    "\n",
    "re_weight = True\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer 만들기\n",
    "\n",
    "embedding_layer = Embedding(nb_words, \n",
    "                                                   EMBEDDING_DIM, \n",
    "                                                   weights=[embedding_matrix], \n",
    "                                                   input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input을 embedding layer에 매핑시킨 후 해당 결과 lstm 인풋에 넣기\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input을 embedding layer에 매핑시킨 후 해당 결과 lstm 인풋에 넣기\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM으로 나온 결과 merge -> dropout -> BN -> FC로 만든 뒤 -> sigmoid\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation='relu')(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = '../../Kaggle_IO/QuoraQuestionPairs/submission/lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/200\n",
      "727722/727722 [==============================] - 109s 150us/step - loss: 0.3795 - acc: 0.6998 - val_loss: 0.3421 - val_acc: 0.7125\n",
      "Epoch 2/200\n",
      "727722/727722 [==============================] - 106s 146us/step - loss: 0.3352 - acc: 0.7292 - val_loss: 0.3245 - val_acc: 0.7662\n",
      "Epoch 3/200\n",
      "727722/727722 [==============================] - 107s 147us/step - loss: 0.3123 - acc: 0.7509 - val_loss: 0.3079 - val_acc: 0.7775\n",
      "Epoch 4/200\n",
      "727722/727722 [==============================] - 108s 149us/step - loss: 0.2941 - acc: 0.7684 - val_loss: 0.2965 - val_acc: 0.7954\n",
      "Epoch 5/200\n",
      "727722/727722 [==============================] - 106s 146us/step - loss: 0.2797 - acc: 0.7831 - val_loss: 0.2936 - val_acc: 0.8098\n",
      "Epoch 6/200\n",
      "727722/727722 [==============================] - 106s 146us/step - loss: 0.2663 - acc: 0.7965 - val_loss: 0.2746 - val_acc: 0.8008\n",
      "Epoch 7/200\n",
      "727722/727722 [==============================] - 106s 145us/step - loss: 0.2559 - acc: 0.8068 - val_loss: 0.2837 - val_acc: 0.8217\n",
      "Epoch 8/200\n",
      "727722/727722 [==============================] - 106s 145us/step - loss: 0.2459 - acc: 0.8160 - val_loss: 0.2815 - val_acc: 0.8262\n",
      "Epoch 9/200\n",
      "727722/727722 [==============================] - 106s 145us/step - loss: 0.2379 - acc: 0.8240 - val_loss: 0.2806 - val_acc: 0.8305\n",
      "Epoch 10/200\n",
      "727722/727722 [==============================] - 106s 145us/step - loss: 0.2300 - acc: 0.8310 - val_loss: 0.2765 - val_acc: 0.8350\n",
      "Epoch 11/200\n",
      "727722/727722 [==============================] - 106s 145us/step - loss: 0.2235 - acc: 0.8369 - val_loss: 0.2771 - val_acc: 0.8375\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv(STAMP+'%.4f_'%(bst_val_score)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".31475 (1094 / 3307, 33%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
