{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2 데이터에 pretrained word vector 적용, 다양한 거리 지표로 변수 생성 후 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import feat_gen # 변수 생성 펑션 파일\n",
    "import importlib; importlib.reload(feat_gen)\n",
    "import pickle\n",
    "import gc\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pyemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import feat_gen\n",
    "import importlib; importlib.reload(feat_gen)\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "import time\n",
    "from dask.dataframe import from_pandas\n",
    "import dask\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1에서 만든 데이터 로드하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F4.pickle', 'rb')\n",
    "train_df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F4.pickle', 'rb')\n",
    "test_df = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vector 활용하기 위한 함수 호출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    \n",
    "    # 문자 소문자로 변환, 공란으로 구분\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    \n",
    "     # 영어 stopwords 불러오기\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # 단어단위로 쪼개서 list 만들기\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    \n",
    "    # 두 단어의 Word Mover's Distance 구함\n",
    "    # 자세한 사항은 아래 참고\n",
    "    # Ofir Pele and Michael Werman, \"A linear time histogram metric for improved SIFT matching\".\n",
    "    # Ofir Pele and Michael Werman, \"Fast and robust earth mover's distances\".\n",
    "    # Matt Kusner et al. \"From Word Embeddings To Document Distances\".\n",
    "    \n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_wmd(s1, s2) :\n",
    "    \n",
    "    # wmd 함수와 똑같지만 norm_model을 활용해서 거리를 구함\n",
    "    # model의 KeyedVector를 호출한 후 init_sims 옵션으로 L2-normalizer 수행한 벡터로 거리 구함\n",
    "    \n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    \n",
    "    return norm_model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    \n",
    "    # pretrained word vector 값을 단어에 매핑시켜서 변수화하는 함수\n",
    "    \n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    \n",
    "    \n",
    "    # 다 알겠는데 제곱합의 sqrt값으로 나눠주는 것의 의미는...잘 와닿진 않는다.\n",
    "    # 보통 평균을 내지 않나...?\n",
    "    \n",
    "    return v / np.sqrt((v**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 벡터를 로딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors. \\\n",
    "              load_word2vec_format('../../../Analysis/WordVectors/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word mover's distance 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오래 걸리는 관계로 pass...병렬처리가 꼭 필요한듯\n",
    "\n",
    "# train_df['wmd'] = train_df.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "# test_df['wmd'] = test_df.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표준화한 모델로 word mover's distance 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_model = model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얘도 오래 걸려서 pass...\n",
    "\n",
    "# train_df['wmd_norm'] = train_df.apply(lambda x: wmd_norm(x['question1'], x['question2']), axis=1)\n",
    "# test_df['wmd_norm'] = test_df.apply(lambda x: wmd_norm(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec 모형 매핑하여 각종 거리 계산한 변수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매핑시킬 array를 미리 만들어두고\n",
    "question1_vectors = np.zeros((train_df.shape[0], 300))\n",
    "question2_vectors = np.zeros((train_df.shape[0], 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set completed\n"
     ]
    }
   ],
   "source": [
    "# 하나씩 매핑시킵니다.\n",
    "for i, q in enumerate(train_df.question1.values):\n",
    "    question1_vectors[i,:] = sent2vec(q)\n",
    "    \n",
    "print(\"train set completed\")\n",
    "    \n",
    "for i, q in enumerate(train_df.question2.values):\n",
    "    question2_vectors[i,:] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n",
      "fin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:763: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n",
      "fin\n",
      "fin\n",
      "fin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:980: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return l1_diff.sum() / l1_sum.sum()\n"
     ]
    }
   ],
   "source": [
    "# 다양한 거리 지표들을 구합니다.\n",
    "\n",
    "train_df['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "train_df['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "train_df['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "train_df['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "train_df['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "train_df['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "train_df['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 word vector의 왜도 및 첨도를 변수화합니다.\n",
    "\n",
    "train_df['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "train_df['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "train_df['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "train_df['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec 모형 매핑하여 각종 거리 계산한 변수 만들기 (test셋에 대해서도 동일하게 수행해줍니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매핑시킬 array를 미리 만들어두고\n",
    "question1_vectors = np.zeros((test_df.shape[0], 300))\n",
    "question2_vectors = np.zeros((test_df.shape[0], 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set completed\n"
     ]
    }
   ],
   "source": [
    "# 하나씩 매핑시킵니다.\n",
    "for i, q in enumerate(test_df.question1.values):\n",
    "    question1_vectors[i,:] = sent2vec(q)\n",
    "    \n",
    "print(\"train set completed\")\n",
    "    \n",
    "for i, q in enumerate(test_df.question2.values):\n",
    "    question2_vectors[i,:] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n",
      "fin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:763: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n",
      "fin\n",
      "fin\n",
      "fin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:980: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return l1_diff.sum() / l1_sum.sum()\n"
     ]
    }
   ],
   "source": [
    "# 다양한 거리 지표들을 구합니다.\n",
    "\n",
    "test_df['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "test_df['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "test_df['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "test_df['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "test_df['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "test_df['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "print(\"fin\")\n",
    "\n",
    "test_df['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 word vector의 왜도 및 첨도를 변수화합니다.\n",
    "\n",
    "test_df['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "test_df['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "test_df['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "test_df['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data....\n"
     ]
    }
   ],
   "source": [
    "# print('saving data....')\n",
    "# f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F5.pickle', 'wb') \n",
    "# pickle.dump(train_df, f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F5.pickle', 'wb') \n",
    "# pickle.dump(test_df, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지금까지 만든 변수로 모형을 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F5.pickle', 'rb')\n",
    "train_df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F5.pickle', 'rb')\n",
    "test_df = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id', 'is_duplicate', 'qid1', 'qid2', 'wmd'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train에만 있는 변수 확인\n",
    "set(train_df.columns) - set(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target변수 새로 생성\n",
    "train_df_target = train_df.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train에만 있는 쓸모없는 변수 제거\n",
    "train_df.drop([\"id\", \"is_duplicate\", \"qid1\", \"qid2\", 'wmd'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test id 제거\n",
    "test_df.drop(['test_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 text 제거\n",
    "train_df.drop(['question1', 'question2'], inplace=True, axis=1)\n",
    "test_df.drop(['question1', 'question2'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 text 제거\n",
    "train_df.drop(['q1_clean1', 'q2_clean1'], inplace=True, axis=1)\n",
    "test_df.drop(['q1_clean1', 'q2_clean1'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**비율 조정하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos = train_df[train_df_target==1]\n",
    "df_train_neg = train_df[train_df_target==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg = pd.concat([df_train_neg, df_train_neg], axis=0)\n",
    "df_train_neg = pd.concat([df_train_neg, df_train_neg.iloc[:210000,:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([df_train_pos, df_train_neg])\n",
    "train_df_target = Series((np.zeros(len(df_train_pos)) + 1).tolist() + np.zeros(len(df_train_neg)).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Xgboost 적합해보기__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정하기\n",
    "\n",
    "params = {\"learning_rate\" : 0.1,\n",
    "                 \"n_estimators\" : 10000,\n",
    "                  \"max_depth\" : 7,\n",
    "                  \"colsample_bytree\" : 0.8,\n",
    "                  \"gamma\" : 2,\n",
    "                  \"lambda\" : 0.5,\n",
    "                  'alpha' : 0.5,\n",
    "                  \"n_jobs\" : 14,\n",
    "#                   \"tree_method\" : 'gpu_hist',\n",
    "#                    \"predictor\" : 'gpu_predictor',\n",
    "                  \"objective\" : \"binary:logistic\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df, train_df_target, test_size = 0.6, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train,\n",
    "        eval_set = [(X_train,y_train), (X_test,y_test)],\n",
    "                            eval_metric='logloss',\n",
    "                            early_stopping_rounds=100,\n",
    "                            verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# single model submission \n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df, ntree_limit=1000)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_v3_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM 시도해보기 _ 별 차이 없었음. 결국 변수 문제.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"learning_rate\" : 0.02,\n",
    "                                             \"max_depth\" : 7,\n",
    "                                             \"bagging_fraction\" : 1,\n",
    "                                             'feature_fraction' : 0.8,\n",
    "                                             'objective': 'binary', \n",
    "                                             'boosting_type': 'gbdt',\n",
    "                                             'metric' : {'binary_logloss'},\n",
    "                                             'device' : 'gpu',\n",
    "                                             'verbose' : 1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.240577\n",
      "[400]\tvalid_0's binary_logloss: 0.226466\n",
      "[600]\tvalid_0's binary_logloss: 0.221363\n",
      "[800]\tvalid_0's binary_logloss: 0.218743\n",
      "[1000]\tvalid_0's binary_logloss: 0.216753\n",
      "[1200]\tvalid_0's binary_logloss: 0.215136\n",
      "[1400]\tvalid_0's binary_logloss: 0.213787\n",
      "[1600]\tvalid_0's binary_logloss: 0.212616\n",
      "[1800]\tvalid_0's binary_logloss: 0.211568\n",
      "[2000]\tvalid_0's binary_logloss: 0.210528\n",
      "[2200]\tvalid_0's binary_logloss: 0.20953\n",
      "[2400]\tvalid_0's binary_logloss: 0.208618\n",
      "[2600]\tvalid_0's binary_logloss: 0.207772\n",
      "[2800]\tvalid_0's binary_logloss: 0.206939\n",
      "[3000]\tvalid_0's binary_logloss: 0.206203\n",
      "[3200]\tvalid_0's binary_logloss: 0.205471\n",
      "[3400]\tvalid_0's binary_logloss: 0.204775\n",
      "[3600]\tvalid_0's binary_logloss: 0.204114\n",
      "[3800]\tvalid_0's binary_logloss: 0.203455\n",
      "[4000]\tvalid_0's binary_logloss: 0.202826\n",
      "[4200]\tvalid_0's binary_logloss: 0.202169\n",
      "[4400]\tvalid_0's binary_logloss: 0.201558\n",
      "[4600]\tvalid_0's binary_logloss: 0.201013\n",
      "[4800]\tvalid_0's binary_logloss: 0.200436\n",
      "[5000]\tvalid_0's binary_logloss: 0.199923\n",
      "[5200]\tvalid_0's binary_logloss: 0.199419\n",
      "[5400]\tvalid_0's binary_logloss: 0.19892\n",
      "[5600]\tvalid_0's binary_logloss: 0.198413\n",
      "[5800]\tvalid_0's binary_logloss: 0.197926\n",
      "[6000]\tvalid_0's binary_logloss: 0.19747\n",
      "[6200]\tvalid_0's binary_logloss: 0.197032\n",
      "[6400]\tvalid_0's binary_logloss: 0.196564\n",
      "[6600]\tvalid_0's binary_logloss: 0.196115\n",
      "[6800]\tvalid_0's binary_logloss: 0.195682\n",
      "[7000]\tvalid_0's binary_logloss: 0.195264\n",
      "[7200]\tvalid_0's binary_logloss: 0.194845\n",
      "[7400]\tvalid_0's binary_logloss: 0.194462\n",
      "[7600]\tvalid_0's binary_logloss: 0.194089\n",
      "[7800]\tvalid_0's binary_logloss: 0.193683\n",
      "[8000]\tvalid_0's binary_logloss: 0.193335\n",
      "[8200]\tvalid_0's binary_logloss: 0.192979\n",
      "[8400]\tvalid_0's binary_logloss: 0.192587\n",
      "[8600]\tvalid_0's binary_logloss: 0.192279\n",
      "[8800]\tvalid_0's binary_logloss: 0.191941\n",
      "[9000]\tvalid_0's binary_logloss: 0.191629\n",
      "[9200]\tvalid_0's binary_logloss: 0.191299\n",
      "[9400]\tvalid_0's binary_logloss: 0.190968\n",
      "[9600]\tvalid_0's binary_logloss: 0.190671\n",
      "[9800]\tvalid_0's binary_logloss: 0.190389\n",
      "[10000]\tvalid_0's binary_logloss: 0.190103\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's binary_logloss: 0.190103\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.train(params, \n",
    "                                        lgb_train,\n",
    "#                                         device='gpu',\n",
    "                                        num_boost_round=10000,\n",
    "                                        valid_sets=lgb_eval,\n",
    "                                        early_stopping_rounds=200,\n",
    "                                        verbose_eval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model submission \n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict(test_df, num_iteration=1000)\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_lgb_v3_5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
