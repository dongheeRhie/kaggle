{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1 모델에 Countvectorizer, Tfidfvectorizer로 변수 생성, 해당 변수로 적합한 naive_bayes 모형의 예측값을 input으로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feat_gen # 변수 생성 펑션 파일\n",
    "import importlib; importlib.reload(feat_gen)\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import feat_gen\n",
    "import importlib; importlib.reload(feat_gen)\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1에서 만든 데이터 로드하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F3.pickle', 'rb')\n",
    "train_df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F3.pickle', 'rb')\n",
    "test_df = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer로 변수 추가하여 모형 개발해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer 변수 생성하기 _ Char Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최고 단어 개수를 지정하고, 1~4개 개수로 Ngram 변수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNumFeatures = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BagOfWordsExtractor1 = CountVectorizer(max_df=0.999, min_df=1000, max_features = maxNumFeatures,\n",
    "                                                                   analyzer = 'char', ngram_range = (1,1), binary = True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor2 = CountVectorizer(max_df=0.999, min_df=1000, max_features =  maxNumFeatures,\n",
    "                                                                  analyzer = 'char', ngram_range = (2,2), binary = True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor3 = CountVectorizer(max_df=0.999, min_df=1000, max_features =  maxNumFeatures,\n",
    "                                                                  analyzer = 'char', ngram_range = (3,3), binary = True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor1234 = CountVectorizer(max_df=0.999, min_df=1000, max_features =  maxNumFeatures,\n",
    "                                                                  analyzer = 'char', ngram_range = (1,4), binary = True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_ExList = [BagOfWordsExtractor1,BagOfWordsExtractor2,BagOfWordsExtractor3,BagOfWordsExtractor1234]\n",
    "BOW_labels = ['1','2','3','1234']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcol = ['question1', 'question2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 모델로 Sparse Matrix 만든 후, <br>\n",
    "두 질문의 Sparse Matrix의 합 (두 테이블에 모두 있을 경우 2, 하나만 있으면 1, 둘다 없으면 0) 및 <br>\n",
    "곱 (둘다 있으면 1, 아니면 0) 으로 Naive_Bayse 모형 만들어서 그 예측값을 변수로 넣기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for Step 0\n",
      "Fitting model for Step 0\n",
      "Preparing data for Step 1\n",
      "Fitting model for Step 1\n",
      "Preparing data for Step 2\n",
      "Fitting model for Step 2\n",
      "Preparing data for Step 3\n",
      "Fitting model for Step 3\n"
     ]
    }
   ],
   "source": [
    "for i, extr in enumerate(BOW_ExList):\n",
    "    \n",
    "    print(\"Preparing data for Step {}\".format(str(i)))\n",
    "    \n",
    "    extr.fit(pd.concat([train_df[qcol[0]], train_df[qcol[1]]]).unique())\n",
    "    BOW_train_q1 = extr.transform(train_df[qcol[0]])\n",
    "    BOW_train_q2 = extr.transform(train_df[qcol[1]])\n",
    "    BOW_test_q1 = extr.transform(test_df[qcol[0]])\n",
    "    BOW_test_q2 = extr.transform(test_df[qcol[1]])\n",
    "    \n",
    "    BOW_add = BOW_train_q1 + BOW_train_q2\n",
    "    BOW_intersec = BOW_train_q1.multiply(BOW_train_q2).sign()\n",
    "    \n",
    "    BOW_add_test = BOW_test_q1 + BOW_test_q2\n",
    "    BOW_intersec_test = BOW_test_q1.multiply(BOW_test_q2).sign()\n",
    "    \n",
    "    del BOW_train_q1, BOW_train_q2, BOW_test_q1, BOW_test_q2 \n",
    "    \n",
    "    print(\"Fitting model for Step {}\".format(str(i)))\n",
    "    \n",
    "    model = MultinomialNB(alpha=1)\n",
    "    y_data = train_df['is_duplicate'].values\n",
    " \n",
    "    model.fit(BOW_add, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_add'] = model.predict_proba(BOW_add)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_add'] = model.predict_proba(BOW_add_test)[:,1]\n",
    "    \n",
    "    model.fit(BOW_intersec, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_inter'] = model.predict_proba(BOW_intersec)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_inter'] = model.predict_proba(BOW_intersec_test)[:,1]\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어단위로 CountVectorizer 수행 후 NB 적합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_extr_word1 = CountVectorizer(max_df=0.999, min_df=1000, max_features=maxNumFeatures, \n",
    "                                      analyzer='word', ngram_range=(1,3), \n",
    "                                      binary=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_ExList = [BOW_extr_word1]\n",
    "BOW_labels = ['1']\n",
    "\n",
    "qcol = ['q1_clean1', 'q2_clean1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for Step 0\n",
      "Fitting model for Step 0\n"
     ]
    }
   ],
   "source": [
    "for i, extr in enumerate(BOW_ExList):\n",
    "    \n",
    "    print(\"Preparing data for Step {}\".format(str(i)))\n",
    "    \n",
    "    extr.fit(pd.concat([train_df[qcol[0]], train_df[qcol[1]]]).unique())\n",
    "    BOW_train_q1 = extr.transform(train_df[qcol[0]])\n",
    "    BOW_train_q2 = extr.transform(train_df[qcol[1]])\n",
    "    BOW_test_q1 = extr.transform(test_df[qcol[0]])\n",
    "    BOW_test_q2 = extr.transform(test_df[qcol[1]])\n",
    "    \n",
    "    BOW_add = BOW_train_q1 + BOW_train_q2\n",
    "    BOW_intersec = BOW_train_q1.multiply(BOW_train_q2).sign()\n",
    "    \n",
    "    BOW_add_test = BOW_test_q1 + BOW_test_q2\n",
    "    BOW_intersec_test = BOW_test_q1.multiply(BOW_test_q2).sign()\n",
    "    \n",
    "    del BOW_train_q1, BOW_train_q2, BOW_test_q1, BOW_test_q2 \n",
    "    \n",
    "    print(\"Fitting model for Step {}\".format(str(i)))\n",
    "    \n",
    "    model = MultinomialNB(alpha=1)\n",
    "    y_data = train_df['is_duplicate'].values\n",
    " \n",
    "    model.fit(BOW_add, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_add'] = model.predict_proba(BOW_add)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_add'] = model.predict_proba(BOW_add_test)[:,1]\n",
    "    \n",
    "    model.fit(BOW_intersec, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_inter'] = model.predict_proba(BOW_intersec)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_inter'] = model.predict_proba(BOW_intersec_test)[:,1]\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer로 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNumFeatures = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BagOfWordsExtractor1 = TfidfVectorizer(max_df=0.999, min_df=1000, max_features = maxNumFeatures,\n",
    "                                                                   analyzer = 'char', ngram_range = (1,1), binary = True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor2 = TfidfVectorizer(max_df=0.999, min_df=1000, max_features =  maxNumFeatures,\n",
    "                                                                  analyzer = 'char', ngram_range = (2,2), binary = True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor3 = TfidfVectorizer(max_df=0.999, min_df=1000, max_features =  maxNumFeatures,\n",
    "                                                                  analyzer = 'char', ngram_range = (3,3), binary = True, lowercase=True)\n",
    "\n",
    "BagOfWordsExtractor1234 = TfidfVectorizer(max_df=0.999, min_df=1000, max_features =  maxNumFeatures,\n",
    "                                                                  analyzer = 'char', ngram_range = (1,4), binary = True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_ExList = [BagOfWordsExtractor1,BagOfWordsExtractor2,BagOfWordsExtractor3,BagOfWordsExtractor1234]\n",
    "BOW_labels = ['1','2','3','1234']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcol = ['question1', 'question2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for Step 0\n",
      "Fitting model for Step 0\n",
      "Preparing data for Step 1\n",
      "Fitting model for Step 1\n",
      "Preparing data for Step 2\n",
      "Fitting model for Step 2\n",
      "Preparing data for Step 3\n",
      "Fitting model for Step 3\n"
     ]
    }
   ],
   "source": [
    "for i, extr in enumerate(BOW_ExList):\n",
    "    \n",
    "    print(\"Preparing data for Step {}\".format(str(i)))\n",
    "    \n",
    "    extr.fit(pd.concat([train_df[qcol[0]], train_df[qcol[1]]]).unique())\n",
    "    BOW_train_q1 = extr.transform(train_df[qcol[0]])\n",
    "    BOW_train_q2 = extr.transform(train_df[qcol[1]])\n",
    "    BOW_test_q1 = extr.transform(test_df[qcol[0]])\n",
    "    BOW_test_q2 = extr.transform(test_df[qcol[1]])\n",
    "    \n",
    "    BOW_add = BOW_train_q1 + BOW_train_q2\n",
    "    BOW_intersec = BOW_train_q1.multiply(BOW_train_q2).sign()\n",
    "    \n",
    "    BOW_add_test = BOW_test_q1 + BOW_test_q2\n",
    "    BOW_intersec_test = BOW_test_q1.multiply(BOW_test_q2).sign()\n",
    "    \n",
    "    del BOW_train_q1, BOW_train_q2, BOW_test_q1, BOW_test_q2 \n",
    "    \n",
    "    print(\"Fitting model for Step {}\".format(str(i)))\n",
    "    \n",
    "    model = MultinomialNB(alpha=1)\n",
    "    y_data = train_df['is_duplicate'].values\n",
    " \n",
    "    model.fit(BOW_add, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_add_tfidf'] = model.predict_proba(BOW_add)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_add_tfidf'] = model.predict_proba(BOW_add_test)[:,1]\n",
    "    \n",
    "    model.fit(BOW_intersec, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_inter_tfidf'] = model.predict_proba(BOW_intersec)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_inter_tfidf'] = model.predict_proba(BOW_intersec_test)[:,1]\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어단위로 TfidfVectorizer 수행 후 NB 적합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_extr_word1 = TfidfVectorizer(max_df=0.999, min_df=1000, max_features=maxNumFeatures, \n",
    "                                      analyzer='word', ngram_range=(1,3), \n",
    "                                      binary=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_ExList = [BOW_extr_word1]\n",
    "BOW_labels = ['1']\n",
    "\n",
    "qcol = ['q1_clean1', 'q2_clean1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for Step 0\n",
      "Fitting model for Step 0\n"
     ]
    }
   ],
   "source": [
    "for i, extr in enumerate(BOW_ExList):\n",
    "    \n",
    "    print(\"Preparing data for Step {}\".format(str(i)))\n",
    "    \n",
    "    extr.fit(pd.concat([train_df[qcol[0]], train_df[qcol[1]]]).unique())\n",
    "    BOW_train_q1 = extr.transform(train_df[qcol[0]])\n",
    "    BOW_train_q2 = extr.transform(train_df[qcol[1]])\n",
    "    BOW_test_q1 = extr.transform(test_df[qcol[0]])\n",
    "    BOW_test_q2 = extr.transform(test_df[qcol[1]])\n",
    "    \n",
    "    BOW_add = BOW_train_q1 + BOW_train_q2\n",
    "    BOW_intersec = BOW_train_q1.multiply(BOW_train_q2).sign()\n",
    "    \n",
    "    BOW_add_test = BOW_test_q1 + BOW_test_q2\n",
    "    BOW_intersec_test = BOW_test_q1.multiply(BOW_test_q2).sign()\n",
    "    \n",
    "    del BOW_train_q1, BOW_train_q2, BOW_test_q1, BOW_test_q2 \n",
    "    \n",
    "    print(\"Fitting model for Step {}\".format(str(i)))\n",
    "    \n",
    "    model = MultinomialNB(alpha=1)\n",
    "    y_data = train_df['is_duplicate'].values\n",
    " \n",
    "    model.fit(BOW_add, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_add_tfidf'] = model.predict_proba(BOW_add)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_add_tfidf'] = model.predict_proba(BOW_add_test)[:,1]\n",
    "    \n",
    "    model.fit(BOW_intersec, y_data)\n",
    "    \n",
    "    train_df['nBayes' + BOW_labels[i] + '_inter_tfidf'] = model.predict_proba(BOW_intersec)[:,1]\n",
    "    test_df['nBayes' + BOW_labels[i] + '_inter_tfidf'] = model.predict_proba(BOW_intersec_test)[:,1]\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data....\n"
     ]
    }
   ],
   "source": [
    "# print('saving data....')\n",
    "# f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F4.pickle', 'wb') \n",
    "# pickle.dump(train_df, f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F4.pickle', 'wb') \n",
    "# pickle.dump(test_df, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/train_F4.pickle', 'rb')\n",
    "train_df = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../../../Kaggle_IO/QuoraQuestionPairs/input/test_F4.pickle', 'rb')\n",
    "test_df = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지금까지 만든 변수로 모형을 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id', 'is_duplicate', 'qid1', 'qid2'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train에만 있는 변수 확인\n",
    "set(train_df.columns) - set(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target변수 새로 생성\n",
    "train_df_target = train_df.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train에만 있는 쓸모없는 변수 제거\n",
    "train_df.drop([\"id\", \"is_duplicate\", \"qid1\", \"qid2\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test id 제거\n",
    "test_df.drop(['test_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 text 제거\n",
    "train_df.drop(['question1', 'question2'], inplace=True, axis=1)\n",
    "test_df.drop(['question1', 'question2'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 text 제거\n",
    "train_df.drop(['q1_clean1', 'q2_clean1'], inplace=True, axis=1)\n",
    "test_df.drop(['q1_clean1', 'q2_clean1'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**비율 조정하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos = train_df[train_df_target==1]\n",
    "df_train_neg = train_df[train_df_target==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg = pd.concat([df_train_neg, df_train_neg], axis=0)\n",
    "df_train_neg = pd.concat([df_train_neg, df_train_neg.iloc[:210000,:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([df_train_pos, df_train_neg])\n",
    "train_df_target = Series((np.zeros(len(df_train_pos)) + 1).tolist() + np.zeros(len(df_train_neg)).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Xgboost 적합해보기__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정하기\n",
    "\n",
    "params = {\"learning_rate\" : 0.1,\n",
    "                 \"n_estimators\" : 1000,\n",
    "                  \"max_depth\" : 5,\n",
    "                  \"colsample_bytree\" : 0.4,\n",
    "                  \"gamma\" : 2,\n",
    "                  \"lambda\" : 0.5,\n",
    "                  'alpha' : 0.5,\n",
    "                  \"n_jobs\" : 14,\n",
    "                  \"objective\" : \"binary:logistic\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df, train_df_target, test_size = 0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.634719\tvalidation_1-logloss:0.634824\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-logloss:0.228469\tvalidation_1-logloss:0.231251\n",
      "[200]\tvalidation_0-logloss:0.216763\tvalidation_1-logloss:0.221835\n",
      "[300]\tvalidation_0-logloss:0.210004\tvalidation_1-logloss:0.217481\n",
      "[400]\tvalidation_0-logloss:0.204438\tvalidation_1-logloss:0.214212\n",
      "[500]\tvalidation_0-logloss:0.200048\tvalidation_1-logloss:0.211913\n",
      "[600]\tvalidation_0-logloss:0.19632\tvalidation_1-logloss:0.210042\n",
      "[700]\tvalidation_0-logloss:0.192664\tvalidation_1-logloss:0.208199\n",
      "[800]\tvalidation_0-logloss:0.189497\tvalidation_1-logloss:0.206674\n",
      "[900]\tvalidation_0-logloss:0.186206\tvalidation_1-logloss:0.205141\n",
      "[999]\tvalidation_0-logloss:0.183117\tvalidation_1-logloss:0.203768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.5, base_score=0.5, booster='gbtree',\n",
       "       colsample_bylevel=1, colsample_bytree=0.4, gamma=2, lambda=0.5,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, n_jobs=14,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train,\n",
    "        eval_set = [(X_train,y_train), (X_test,y_test)],\n",
    "                            eval_metric='logloss',\n",
    "                            early_stopping_rounds=200,\n",
    "                            verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# single model submission, 0.21059, 729th, 상위 22%\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model submission (count, tfidf 추가, 3000회 학습후 종료), 0.21262, 오히려 더 떨어진..오버피팅인가?\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model submission (count, tfidf 추가, 3000회 학습후 종료), 0.23391, 오버피팅인듯\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model submission (count, tfidf 추가, 3000회 학습후 종료), 300개 tree만 활용, 0.21224\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df, ntree_limit=300)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model submission (count, tfidf 추가, 3000회 학습후 종료), 100개 tree만 활용, 0.20767, 732th\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df, ntree_limit=100)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model submission (count, tfidf 추가), 0.20424, 729th\n",
    "# 파라미터 조정 후 재적합 ( depth 5, rate 0.4)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = range(test_df.shape[0])\n",
    "sub['is_duplicate'] = clf.predict_proba(test_df)[:,1]\n",
    "sub.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 두개 모형 ensemble, 0.20402, 729th\n",
    "\n",
    "pred1= pd.read_csv(\"../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_5.csv\")\n",
    "pred2= pd.read_csv(\"../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_6.csv\")\n",
    "\n",
    "pred3 = DataFrame()\n",
    "pred3['test_id'] = pred2['test_id']\n",
    "pred3['is_duplicate'] = (pred1.is_duplicate * pred2.is_duplicate)**(1/2)\n",
    "pred3.to_csv('../../../Kaggle_IO/QuoraQuestionPairs/submission/addfeature_xgb_7.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
